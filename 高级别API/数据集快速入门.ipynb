{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集快速入门\n",
    "tf.data 模块包含一系列类，可让您轻松地加载数据、操作数据并通过管道将数据传送到模型中。本文档通过两个简单的示例来介绍该 API：\n",
    "\n",
    "从 Numpy 数组中读取内存中的数据。\n",
    "\n",
    "从 csv 文件中读取行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数\n",
    "此函数需要三个参数。要求所赋值为“数组”的参数能够接受可通过 numpy.array 转换成数组的几乎任何值。其中存在一个例外，即对 Datasets 有特殊意义的 tuple，稍后我们会发现这一点。\n",
    "\n",
    "features：包含原始输入特征的 {'feature_name':array} 字典（或 DataFrame）。\n",
    "\n",
    "labels：包含每个样本的标签的数组。\n",
    "\n",
    "batch_size：表示所需批次大小的整数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切片\n",
    "首先，此函数会利用 tf.data.Dataset.from_tensor_slices 函数创建一个代表数组切片的 tf.data.Dataset。系统会在第一个维度内对该数组进行切片。例如，一个包含 mnist 训练数据的数组的形状为 (60000, 28, 28)。将该数组传递给 from_tensor_slices 会返回一个包含 60000 个切片的 Dataset 对象，其中每个切片都是一个 28x28 的图像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (28, 28), types: tf.uint8>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "train,test = tf.keras.datasets.mnist.load_data() \n",
    "mnist_x,mnist_y = train\n",
    "\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices(mnist_x)\n",
    "print(mnist_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 鸢尾花示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:1.9.0\n",
      "Eager Excution:False\n"
     ]
    }
   ],
   "source": [
    "#版权归https://www.tensorflow.org/get_started/get_started_for_beginners\n",
    "from __future__ import  absolute_import,division,print_function\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow.contrib.eager as tfe\n",
    "import argparse\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', default=100, type=int, help='batch size')\n",
    "parser.add_argument('--train_steps', default=1000, type=int,\n",
    "                    help='number of training steps')\n",
    "\n",
    "print(\"tensorflow version:{}\".format(tf.VERSION))\n",
    "print(\"Eager Excution:{}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "def load_data(label_name = 'Species'):\n",
    "    #训练集提取\n",
    "    train_path = tf.keras.utils.get_file(fname= os.path.basename(TRAIN_URL),\n",
    "                                        origin = TRAIN_URL)\n",
    "    train = pd.read_csv(filepath_or_buffer=train_path,names=CSV_COLUMN_NAMES,header=0)\n",
    "   \n",
    "    train_features,train_label = train,train.pop(label_name)\n",
    "    \n",
    "    #测试集提取\n",
    "    test_path = tf.keras.utils.get_file(fname=os.path.basename(TEST_URL),\n",
    "                                       origin = TEST_URL)\n",
    "    test = pd.read_csv(filepath_or_buffer=test_path,names=CSV_COLUMN_NAMES,header=0)\n",
    "    \n",
    "    test_features,test_label = test,test.pop(label_name)\n",
    "    \n",
    "    return(train_features,train_label),(test_features,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x,train_y),(test_x,test_y) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: {SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, types: {SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(dict(train_x))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((dict(train_x), train_y))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 操作\n",
    "目前，Dataset 会按固定顺序迭代数据一次，并且一次仅生成一个元素。它需要进一步处理才可用于训练。幸运的是，tf.data.Dataset 类提供了更好地准备训练数据的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shuffle、repeat、batch\n",
    "shuffle 方法使用一个固定大小的缓冲区，在条目经过时随机化处理条目。在这种情况下，buffer_size 大于 Dataset 中样本的数量，确保数据完全被随机化处理.\n",
    "\n",
    "repeat 方法会在结束时重启 Dataset。要限制周期数量，请设置 count 参数。\n",
    "\n",
    "batch 方法会收集大量样本并将它们堆叠起来以创建批次。这为批次的形状增加了一个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle, repeat, and batch the examples.\n",
    "dataset = dataset.shuffle(1000).repeat().batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (?, 28, 28), types: tf.uint8>\n"
     ]
    }
   ],
   "source": [
    "print(mnist_ds.batch(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ({SepalLength: (?,), SepalWidth: (?,), PetalLength: (?,), PetalWidth: (?,)}, (?,)), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取CSV文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建 Dataset\n",
    "我们先构建一个 TextLineDataset 对象，实现一次读取文件中的一行数据。然后，我们调用 skip 方法来跳过文件的第一行，此行包含标题，而非样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris_data\n",
    "train_path,test_path = iris_data.maybe_download()\n",
    "ds = tf.data.TextLineDataset(train_path).skip(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建 csv 行解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata describing the text columns\n",
    "COLUMNS = ['SepalLength', 'SepalWidth',\n",
    "           'PetalLength', 'PetalWidth',\n",
    "           'label']\n",
    "FIELD_DEFAULTS = [[0.0],[0.0],[0.0],[0.0],[0]]\n",
    "def _parse_line(line):\n",
    "    #Decode the line into its fields\n",
    "    fields = tf.decode_csv(line,FIELD_DEFAULTS)\n",
    "    \n",
    "    # Pack the result into a dictionary\n",
    "    features = dict(zip(COLUMNS,fields))\n",
    "    \n",
    "    # Separate the label from the features\n",
    "    label = features.pop('label')\n",
    "    \n",
    "    return features,label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析行\n",
    "数据集提供很多用于在通过管道将数据传送到模型的过程中处理数据的方法。最常用的方法是 map，它会对 Dataset 的每个元素应用转换。\n",
    "\n",
    "map 方法会接受 map_func 参数，此参数描述了应该如何转换 Dataset 中的每个条目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float32, SepalWidth: tf.float32, PetalLength: tf.float32, PetalWidth: tf.float32}, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(_parse_line)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 试试看\n",
    "此函数可用于替换 iris_data.train_input_fn。可使用此函数馈送 Estimator，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\THINK\\AppData\\Local\\Temp\\tmpl_qc7fjr\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\THINK\\\\AppData\\\\Local\\\\Temp\\\\tmpl_qc7fjr', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000017C64E55630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\THINK\\AppData\\Local\\Temp\\tmpl_qc7fjr\\model.ckpt.\n",
      "INFO:tensorflow:loss = 109.86121, step = 1\n",
      "INFO:tensorflow:global_step/sec: 238.738\n",
      "INFO:tensorflow:loss = 32.593697, step = 101 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 312.577\n",
      "INFO:tensorflow:loss = 26.917332, step = 201 (0.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.466\n",
      "INFO:tensorflow:loss = 22.905329, step = 301 (0.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.323\n",
      "INFO:tensorflow:loss = 17.174347, step = 401 (0.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 285.181\n",
      "INFO:tensorflow:loss = 17.936905, step = 501 (0.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.419\n",
      "INFO:tensorflow:loss = 16.42101, step = 601 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.705\n",
      "INFO:tensorflow:loss = 17.229847, step = 701 (0.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 312.206\n",
      "INFO:tensorflow:loss = 14.769435, step = 801 (0.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 312.619\n",
      "INFO:tensorflow:loss = 13.920088, step = 901 (0.320 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\THINK\\AppData\\Local\\Temp\\tmpl_qc7fjr\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 15.915887.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x17c64b62b38>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path, test_path = iris_data.maybe_download()\n",
    "\n",
    "# All the inputs are numeric\n",
    "feature_columns = [\n",
    "    tf.feature_column.numeric_column(name)\n",
    "    for name in iris_data.CSV_COLUMN_NAMES[:-1] \n",
    "]\n",
    "\n",
    "#builid the estimator\n",
    "est = tf.estimator.LinearClassifier(feature_columns,\n",
    "                                    n_classes=3)\n",
    "\n",
    "#Train the estimator\n",
    "batch_size = 100\n",
    "est.train(\n",
    "    steps = 1000,\n",
    "    input_fn = lambda:iris_data.csv_input_fn(train_path,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
