{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简介"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "本指南旨在指导您使用低级别 TensorFlow API (TensorFlow Core) 开始编程。您可以学习执行以下操作：\n",
    "\n",
    "管理您自己的 TensorFlow 程序 (tf.Graph) 和 TensorFlow 运行时 (tf.Session)，而不是依靠 Estimator 来管理它们。\n",
    "使用 tf.Session 运行 TensorFlow 操作。\n",
    "在此低级别环境中使用高级别组件（数据集、层和 feature_columns）。\n",
    "构建自己的训练循环，而不是使用 Estimator 提供的训练循环。\n",
    "我们建议尽可能使用更高阶的 API 构建模型。以下是 TensorFlow Core 为何很重要的原因：\n",
    "\n",
    "如果您能够直接使用低阶 TensorFlow 操作，实验和调试都会更直接。\n",
    "在使用更高阶的 API 时，能够理解其内部工作原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import  absolute_import\n",
    "from __future__ import  division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 中的核心数据单位是张量。一个张量由一组形成阵列（任意维数）的原始值组成。张量的阶是它的维数，而它的形状是一个整数元组，指定了阵列每个维度的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.0, 2.0, 3.0]], [[7.0, 8.0, 9.0]]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3. # a rank 0 tensor; a scalar with shape [],\n",
    "[1., 2., 3.] # a rank 1 tensor; a vector with shape [3]\n",
    "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]\n",
    "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Core 演示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以将 TensorFlow Core 程序看作由两个互相独立的部分组成：\n",
    "\n",
    "1.构建计算图 (tf.Graph)。\n",
    "2.运行计算图（使用 tf.Session）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算图是排列成一个图的一系列 TensorFlow 指令。图由两种类型的对象组成。\n",
    "\n",
    "操作（简称“op”）：图的节点。操作描述了消耗和生成张量的计算。\n",
    "\n",
    "张量：图的边。它们代表将流经图的值。大多数 TensorFlow 函数会返回 tf.Tensors。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Const_1:0\", shape=(), dtype=float32)\n",
      "Tensor(\"add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(3.0,dtype=tf.float32)\n",
    "b = tf.constant(4.0)\n",
    "total = a + b\n",
    "print(a)\n",
    "print(b)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中的每个指令都拥有唯一的名称。这个名称不同于使用 Python 分配给相应对象的名称。张量是根据生成它们的指令命名的，后面跟着输出索引，如上文的 \"add:0\" 所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 提供了一个名为 TensorBoard 的实用程序。TensorBoard 的诸多功能之一是将计算图可视化。您只需要使用几个简单的命令就能轻松完成此操作。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "首先将计算图保存为 TensorBoard 摘要文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "这将在当前目录中生成一个 event 文件，其名称格式如下：\n",
    "events.out.tfevents.{timestamp}.{hostname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 会话（Session）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要评估张量，您需要实例化一个 tf.Session 对象（非正式名称为会话）。会话会封装 TensorFlow 运行时的状态，并运行 TensorFlow 操作。如果说 tf.Graph 像一个 .py 文件，那么 tf.Session 就像一个 python 可执行对象。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "下面的代码会创建一个 tf.Session 对象，然后调用其 run 方法来评估我们在上文中创建的 total 张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ab': (3.0, 4.0), 'total': 7.0}\n"
     ]
    }
   ],
   "source": [
    "print(sess.run({'ab':(a,b),\"total\":total}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85014737 0.6020273  0.3850925 ]\n",
      "[0.6006186  0.05520439 0.6114532 ]\n",
      "(array([1.8606313, 1.4063618, 1.3833532], dtype=float32), array([2.8606315, 2.4063618, 2.3833532], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "vec = tf.random_uniform(shape=(3,))\n",
    "out1 = vec + 1\n",
    "out2 = vec + 2\n",
    "print(sess.run(vec))\n",
    "print(sess.run(vec))\n",
    "print(sess.run((out1,out2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 供给"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前来讲，这个图不是特别有趣，因为它总是生成一个常量结果。图可以参数化以便接受外部输入，也称为占位符。占位符表示承诺在稍后提供值，它就像函数参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "使用 run 方法的 feed_dict 参数为占位符提供具体的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(z,feed_dict={x:3,y:4.5}))\n",
    "print(sess.run(z,feed_dict={x:[1,3],y:[2,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "占位符适用于简单的实验，但数据集是将数据流式传输到模型的首选方法。\n",
    "\n",
    "要从数据集中获取可运行的 tf.Tensor，您必须先将其转换成 tf.data.Iterator，然后调用迭代器的 get_next 方法。\n",
    "\n",
    "创建迭代器的最简单的方式是采用 make_one_shot_iterator 方法。例如，在下面的代码中，next_item 张量将在每次 run 调用时从 my_data 阵列返回一行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = [\n",
    "    [0,1,],\n",
    "    [2,3,],\n",
    "    [4,5,],\n",
    "    [6,7,],\n",
    "]\n",
    "slices = tf.data.Dataset.from_tensor_slices(my_data)\n",
    "next_item = slices.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到达数据流末端时，Dataset 会抛出 OutOfRangeError。例如，下面的代码会一直读取 next_item，直到没有数据可读"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[2 3]\n",
      "[4 5]\n",
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        print(sess.run(next_item))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "如果 Dataset 依赖于有状态操作，则可能需要在使用迭代器之前先初始化它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2667524 1.2624987 1.2931088]\n",
      "[-1.3710983 -1.759538  -2.0084453]\n",
      "[ 1.0355133   0.22697236 -0.4193986 ]\n",
      "[-2.276799   0.9500257 -1.0041165]\n",
      "[-0.06327701  0.29767665  0.2769463 ]\n",
      "[ 1.3498988  -1.0641057  -0.18013124]\n",
      "[0.5873358 0.5025349 1.3213618]\n",
      "[-0.76654744 -0.39083168  1.1014665 ]\n",
      "[-0.23919052  0.4992199   0.67420566]\n",
      "[-1.0455903  1.1864312  1.3076091]\n"
     ]
    }
   ],
   "source": [
    "r = tf.random_normal([10,3])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(r)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_row = iterator.get_next()\n",
    "\n",
    "sess.run(iterator.initializer)\n",
    "while True:\n",
    "    try:\n",
    "        print(sess.run(next_row))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层将变量和作用于它们的操作打包在一起。例如，密集连接层会对每个输出对应的所有输入执行加权和，并应用激活函数（可选）。连接权重和偏差由层对象管理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape=[None,3])\n",
    "linear_model = tf.layers.Dense(units=1)\n",
    "y = linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.2522702]\n",
      " [2.5093665]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(y,feed_dict={x:[[1,2,3],[4,5,6]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 层函数的快捷方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0257243]\n",
      " [2.5385725]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32,shape=[None,3])\n",
    "y =  tf.layers.dense(x,units=1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(y,feed_dict={x:[[1,2,3],[4,5,6]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用特征列进行实验的最简单方法是使用 tf.feature_column.input_layer 函数。此函数只接受密集列作为输入，因此要查看类别列的结果，您必须将其封装在 tf.feature_column.indicator_column 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    'sales' : [[5], [10], [8], [9]],\n",
    "    'department': ['sports', 'sports', 'gardening', 'gardening']}\n",
    "\n",
    "department_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'department', ['sports', 'gardening'])\n",
    "department_column = tf.feature_column.indicator_column(department_column)\n",
    "\n",
    "columns = [\n",
    "    tf.feature_column.numeric_column('sales'),\n",
    "    department_column\n",
    "]\n",
    "\n",
    "inputs = tf.feature_column.input_layer(features, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行 inputs 张量会将 features 解析为一批向量。\n",
    "\n",
    "特征列和层一样具有内部状态，因此通常需要将它们初始化。类别列会在内部使用对照表，而这些表需要单独的初始化操作 tf.tables_initializer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_init = tf.global_variables_initializer()\n",
    "table_init = tf.tables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run((var_init,table_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='department', vocabulary_list=('sports', 'gardening'), dtype=tf.string, default_value=-1, num_oov_buckets=0))\n",
      "[[ 1.  0.  5.]\n",
      " [ 1.  0. 10.]\n",
      " [ 0.  1.  8.]\n",
      " [ 0.  1.  9.]]\n"
     ]
    }
   ],
   "source": [
    "print(department_column)\n",
    "print(sess.run(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[1],[2],[3],[4]],dtype=tf.float32)\n",
    "y_true = tf.constant([[0],[-1],[-2],[-3]],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = tf.layers.Dense(units=1)\n",
    "y_pred = linear_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4103967]\n",
      " [2.8207934]\n",
      " [4.23119  ]\n",
      " [5.641587 ]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.52311\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.mean_squared_error(labels=y_true,predictions=y_pred)\n",
    "print(sess.run(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 提供了优化器来执行标准的优化算法。这些优化器被实现为 tf.train.Optimizer 的子类。它们会逐渐改变每个变量，以便将损失最小化。最简单的优化算法是梯度下降法，由 tf.train.GradientDescentOptimizer 实现。它会根据损失相对于变量的导数大小来修改各个变量."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:the loss is 0.022158587351441383\n",
      "epoch 1:the loss is 0.02202611044049263\n",
      "epoch 2:the loss is 0.021894413977861404\n",
      "epoch 3:the loss is 0.02176351472735405\n",
      "epoch 4:the loss is 0.021633388474583626\n",
      "epoch 5:the loss is 0.021504050120711327\n",
      "epoch 6:the loss is 0.02137548103928566\n",
      "epoch 7:the loss is 0.021247677505016327\n",
      "epoch 8:the loss is 0.021120639517903328\n",
      "epoch 9:the loss is 0.02099437266588211\n",
      "epoch 10:the loss is 0.02086884155869484\n",
      "epoch 11:the loss is 0.02074408158659935\n",
      "epoch 12:the loss is 0.02062004990875721\n",
      "epoch 13:the loss is 0.020496759563684464\n",
      "epoch 14:the loss is 0.020374218001961708\n",
      "epoch 15:the loss is 0.02025241032242775\n",
      "epoch 16:the loss is 0.020131327211856842\n",
      "epoch 17:the loss is 0.020010948181152344\n",
      "epoch 18:the loss is 0.01989131048321724\n",
      "epoch 19:the loss is 0.01977238990366459\n",
      "epoch 20:the loss is 0.0196541715413332\n",
      "epoch 21:the loss is 0.019536655396223068\n",
      "epoch 22:the loss is 0.019419850781559944\n",
      "epoch 23:the loss is 0.019303742796182632\n",
      "epoch 24:the loss is 0.019188320264220238\n",
      "epoch 25:the loss is 0.019073599949479103\n",
      "epoch 26:the loss is 0.018959568813443184\n",
      "epoch 27:the loss is 0.018846195191144943\n",
      "epoch 28:the loss is 0.01873352751135826\n",
      "epoch 29:the loss is 0.018621524795889854\n",
      "epoch 30:the loss is 0.01851019077003002\n",
      "epoch 31:the loss is 0.018399512395262718\n",
      "epoch 32:the loss is 0.018289506435394287\n",
      "epoch 33:the loss is 0.018180161714553833\n",
      "epoch 34:the loss is 0.01807146519422531\n",
      "epoch 35:the loss is 0.017963411286473274\n",
      "epoch 36:the loss is 0.017856018617749214\n",
      "epoch 37:the loss is 0.017749259248375893\n",
      "epoch 38:the loss is 0.01764313131570816\n",
      "epoch 39:the loss is 0.01753764972090721\n",
      "epoch 40:the loss is 0.017432786524295807\n",
      "epoch 41:the loss is 0.01732856221497059\n",
      "epoch 42:the loss is 0.01722496561706066\n",
      "epoch 43:the loss is 0.01712197996675968\n",
      "epoch 44:the loss is 0.0170196034014225\n",
      "epoch 45:the loss is 0.016917841508984566\n",
      "epoch 46:the loss is 0.016816698014736176\n",
      "epoch 47:the loss is 0.016716139391064644\n",
      "epoch 48:the loss is 0.01661619544029236\n",
      "epoch 49:the loss is 0.016516858711838722\n",
      "epoch 50:the loss is 0.016418108716607094\n",
      "epoch 51:the loss is 0.016319938004016876\n",
      "epoch 52:the loss is 0.016222374513745308\n",
      "epoch 53:the loss is 0.016125380992889404\n",
      "epoch 54:the loss is 0.016028951853513718\n",
      "epoch 55:the loss is 0.01593313179910183\n",
      "epoch 56:the loss is 0.015837864950299263\n",
      "epoch 57:the loss is 0.015743166208267212\n",
      "epoch 58:the loss is 0.01564904674887657\n",
      "epoch 59:the loss is 0.015555476769804955\n",
      "epoch 60:the loss is 0.015462483279407024\n",
      "epoch 61:the loss is 0.015370032750070095\n",
      "epoch 62:the loss is 0.015278128907084465\n",
      "epoch 63:the loss is 0.015186790376901627\n",
      "epoch 64:the loss is 0.015095988288521767\n",
      "epoch 65:the loss is 0.01500573381781578\n",
      "epoch 66:the loss is 0.0149160111322999\n",
      "epoch 67:the loss is 0.014826837927103043\n",
      "epoch 68:the loss is 0.01473819836974144\n",
      "epoch 69:the loss is 0.014650082215666771\n",
      "epoch 70:the loss is 0.014562482014298439\n",
      "epoch 71:the loss is 0.014475415460765362\n",
      "epoch 72:the loss is 0.01438886672258377\n",
      "epoch 73:the loss is 0.014302841387689114\n",
      "epoch 74:the loss is 0.014217321760952473\n",
      "epoch 75:the loss is 0.014132324606180191\n",
      "epoch 76:the loss is 0.014047831296920776\n",
      "epoch 77:the loss is 0.01396383997052908\n",
      "epoch 78:the loss is 0.01388035248965025\n",
      "epoch 79:the loss is 0.01379736140370369\n",
      "epoch 80:the loss is 0.01371487695723772\n",
      "epoch 81:the loss is 0.01363287027925253\n",
      "epoch 82:the loss is 0.01355136651545763\n",
      "epoch 83:the loss is 0.013470346108078957\n",
      "epoch 84:the loss is 0.01338980346918106\n",
      "epoch 85:the loss is 0.013309742324054241\n",
      "epoch 86:the loss is 0.013230164535343647\n",
      "epoch 87:the loss is 0.01315107848495245\n",
      "epoch 88:the loss is 0.013072436675429344\n",
      "epoch 89:the loss is 0.012994290329515934\n",
      "epoch 90:the loss is 0.012916596606373787\n",
      "epoch 91:the loss is 0.012839370407164097\n",
      "epoch 92:the loss is 0.012762604281306267\n",
      "epoch 93:the loss is 0.01268630102276802\n",
      "epoch 94:the loss is 0.01261044293642044\n",
      "epoch 95:the loss is 0.01253504864871502\n",
      "epoch 96:the loss is 0.01246009673923254\n",
      "epoch 97:the loss is 0.012385599315166473\n",
      "epoch 98:the loss is 0.012311546131968498\n",
      "epoch 99:the loss is 0.012237939983606339\n",
      "[[-0.17809075]\n",
      " [-1.0862972 ]\n",
      " [-1.9945035 ]\n",
      " [-2.90271   ]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    _,loss_value = sess.run((train,loss))\n",
    "    print(\"epoch {}:the loss is {}\".format(epoch,loss_value))\n",
    "    \n",
    "print(sess.run(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6555672\n"
     ]
    }
   ],
   "source": [
    "a = tf.tanh()\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
