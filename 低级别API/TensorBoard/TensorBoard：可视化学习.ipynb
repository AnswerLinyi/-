{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard:可视化学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 可用于训练大规模深度神经网络所需的计算，使用该工具涉及的计算往往复杂而深奥。为了更方便 TensorFlow 程序的理解、调试与优化，我们发布了一套名为 TensorBoard 的可视化工具。您可以用 TensorBoard 来展现 TensorFlow 图，绘制图像生成的定量指标图以及显示附加数据（如其中传递的图像）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import  absolute_import\n",
    "from __future__ import  print_function\n",
    "from __future__ import  division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import  input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-68120a8bc248>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp\\tensorflow/mnist/input_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp\\tensorflow/mnist/input_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp\\tensorflow/mnist/input_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp\\tensorflow/mnist/input_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Accuracy at step 0: 0.1228\n",
      "Accuracy at step 10: 0.7227\n",
      "Accuracy at step 20: 0.8231\n",
      "Accuracy at step 30: 0.8649\n",
      "Accuracy at step 40: 0.8843\n",
      "Accuracy at step 50: 0.8942\n",
      "Accuracy at step 60: 0.902\n",
      "Accuracy at step 70: 0.9006\n",
      "Accuracy at step 80: 0.9112\n",
      "Accuracy at step 90: 0.9139\n",
      "Adding run metadata for 99\n",
      "Accuracy at step 100: 0.9189\n",
      "Accuracy at step 110: 0.9209\n",
      "Accuracy at step 120: 0.9226\n",
      "Accuracy at step 130: 0.9178\n",
      "Accuracy at step 140: 0.9189\n",
      "Accuracy at step 150: 0.9146\n",
      "Accuracy at step 160: 0.9276\n",
      "Accuracy at step 170: 0.9311\n",
      "Accuracy at step 180: 0.9269\n",
      "Accuracy at step 190: 0.9305\n",
      "Adding run metadata for 199\n",
      "Accuracy at step 200: 0.9349\n",
      "Accuracy at step 210: 0.9339\n",
      "Accuracy at step 220: 0.9369\n",
      "Accuracy at step 230: 0.9389\n",
      "Accuracy at step 240: 0.9399\n",
      "Accuracy at step 250: 0.9405\n",
      "Accuracy at step 260: 0.9408\n",
      "Accuracy at step 270: 0.9384\n",
      "Accuracy at step 280: 0.9411\n",
      "Accuracy at step 290: 0.9433\n",
      "Adding run metadata for 299\n",
      "Accuracy at step 300: 0.945\n",
      "Accuracy at step 310: 0.9479\n",
      "Accuracy at step 320: 0.9456\n",
      "Accuracy at step 330: 0.9449\n",
      "Accuracy at step 340: 0.9459\n",
      "Accuracy at step 350: 0.9501\n",
      "Accuracy at step 360: 0.9496\n",
      "Accuracy at step 370: 0.9482\n",
      "Accuracy at step 380: 0.9486\n",
      "Accuracy at step 390: 0.9513\n",
      "Adding run metadata for 399\n",
      "Accuracy at step 400: 0.9474\n",
      "Accuracy at step 410: 0.9533\n",
      "Accuracy at step 420: 0.9549\n",
      "Accuracy at step 430: 0.9512\n",
      "Accuracy at step 440: 0.9524\n",
      "Accuracy at step 450: 0.9552\n",
      "Accuracy at step 460: 0.9523\n",
      "Accuracy at step 470: 0.9507\n",
      "Accuracy at step 480: 0.9528\n",
      "Accuracy at step 490: 0.9564\n",
      "Adding run metadata for 499\n",
      "Accuracy at step 500: 0.9555\n",
      "Accuracy at step 510: 0.9552\n",
      "Accuracy at step 520: 0.9585\n",
      "Accuracy at step 530: 0.9591\n",
      "Accuracy at step 540: 0.9596\n",
      "Accuracy at step 550: 0.9557\n",
      "Accuracy at step 560: 0.9582\n",
      "Accuracy at step 570: 0.9609\n",
      "Accuracy at step 580: 0.9599\n",
      "Accuracy at step 590: 0.959\n",
      "Adding run metadata for 599\n",
      "Accuracy at step 600: 0.9613\n",
      "Accuracy at step 610: 0.9608\n",
      "Accuracy at step 620: 0.9624\n",
      "Accuracy at step 630: 0.964\n",
      "Accuracy at step 640: 0.9635\n",
      "Accuracy at step 650: 0.9588\n",
      "Accuracy at step 660: 0.9618\n",
      "Accuracy at step 670: 0.9577\n",
      "Accuracy at step 680: 0.9618\n",
      "Accuracy at step 690: 0.9625\n",
      "Adding run metadata for 699\n",
      "Accuracy at step 700: 0.9639\n",
      "Accuracy at step 710: 0.9615\n",
      "Accuracy at step 720: 0.9602\n",
      "Accuracy at step 730: 0.9627\n",
      "Accuracy at step 740: 0.9632\n",
      "Accuracy at step 750: 0.9606\n",
      "Accuracy at step 760: 0.9628\n",
      "Accuracy at step 770: 0.9661\n",
      "Accuracy at step 780: 0.9669\n",
      "Accuracy at step 790: 0.9636\n",
      "Adding run metadata for 799\n",
      "Accuracy at step 800: 0.9657\n",
      "Accuracy at step 810: 0.9641\n",
      "Accuracy at step 820: 0.9663\n",
      "Accuracy at step 830: 0.9662\n",
      "Accuracy at step 840: 0.9668\n",
      "Accuracy at step 850: 0.965\n",
      "Accuracy at step 860: 0.9654\n",
      "Accuracy at step 870: 0.9664\n",
      "Accuracy at step 880: 0.9662\n",
      "Accuracy at step 890: 0.9664\n",
      "Adding run metadata for 899\n",
      "Accuracy at step 900: 0.9676\n",
      "Accuracy at step 910: 0.9664\n",
      "Accuracy at step 920: 0.9684\n",
      "Accuracy at step 930: 0.9684\n",
      "Accuracy at step 940: 0.9664\n",
      "Accuracy at step 950: 0.9662\n",
      "Accuracy at step 960: 0.9685\n",
      "Accuracy at step 970: 0.9675\n",
      "Accuracy at step 980: 0.9678\n",
      "Accuracy at step 990: 0.9675\n",
      "Adding run metadata for 999\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "  # Import data\n",
    "  mnist = input_data.read_data_sets(FLAGS.data_dir,\n",
    "                                    fake_data=FLAGS.fake_data)\n",
    "\n",
    "  sess = tf.InteractiveSession()\n",
    "  # Create a multilayer model.\n",
    "\n",
    "  # Input placeholders\n",
    "  with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "    y_ = tf.placeholder(tf.int64, [None], name='y-input')\n",
    "\n",
    "  with tf.name_scope('input_reshape'):\n",
    "    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', image_shaped_input, 10)\n",
    "\n",
    "  # We can't initialize these variables to 0 - the network will get stuck.\n",
    "  def weight_variable(shape):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "  def bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "  def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "      mean = tf.reduce_mean(var)\n",
    "      tf.summary.scalar('mean', mean)\n",
    "      with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "      tf.summary.scalar('stddev', stddev)\n",
    "      tf.summary.scalar('max', tf.reduce_max(var))\n",
    "      tf.summary.scalar('min', tf.reduce_min(var))\n",
    "      tf.summary.histogram('histogram', var)\n",
    "\n",
    "  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer.\n",
    "    It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.\n",
    "    It also sets up name scoping so that the resultant graph is easy to read,\n",
    "    and adds a number of summary ops.\n",
    "    \"\"\"\n",
    "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "    with tf.name_scope(layer_name):\n",
    "      # This Variable will hold the state of the weights for the layer\n",
    "      with tf.name_scope('weights'):\n",
    "        weights = weight_variable([input_dim, output_dim])\n",
    "        variable_summaries(weights)\n",
    "      with tf.name_scope('biases'):\n",
    "        biases = bias_variable([output_dim])\n",
    "        variable_summaries(biases)\n",
    "      with tf.name_scope('Wx_plus_b'):\n",
    "        preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "        tf.summary.histogram('pre_activations', preactivate)\n",
    "      activations = act(preactivate, name='activation')\n",
    "      tf.summary.histogram('activations', activations)\n",
    "      return activations\n",
    "\n",
    "  hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "\n",
    "  with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "    dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "\n",
    "  # Do not apply softmax activation yet, see below.\n",
    "  y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)\n",
    "\n",
    "  with tf.name_scope('cross_entropy'):\n",
    "    # The raw formulation of cross-entropy,\n",
    "    #\n",
    "    # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n",
    "    #                               reduction_indices=[1]))\n",
    "    #\n",
    "    # can be numerically unstable.\n",
    "    #\n",
    "    # So here we use tf.losses.sparse_softmax_cross_entropy on the\n",
    "    # raw logit outputs of the nn_layer above, and then average across\n",
    "    # the batch.\n",
    "    with tf.name_scope('total'):\n",
    "      cross_entropy = tf.losses.sparse_softmax_cross_entropy(\n",
    "          labels=y_, logits=y)\n",
    "  tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "  with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n",
    "        cross_entropy)\n",
    "\n",
    "  with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "      correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
    "    with tf.name_scope('accuracy'):\n",
    "      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "  tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "  # Merge all the summaries and write them out to\n",
    "  # /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)\n",
    "  merged = tf.summary.merge_all()\n",
    "  train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train', sess.graph)\n",
    "  test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test')\n",
    "  tf.global_variables_initializer().run()\n",
    "\n",
    "  # Train the model, and also write summaries.\n",
    "  # Every 10th step, measure test-set accuracy, and write test summaries\n",
    "  # All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "  def feed_dict(train):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if train or FLAGS.fake_data:\n",
    "      xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n",
    "      k = FLAGS.dropout\n",
    "    else:\n",
    "      xs, ys = mnist.test.images, mnist.test.labels\n",
    "      k = 1.0\n",
    "    return {x: xs, y_: ys, keep_prob: k}\n",
    "\n",
    "  for i in range(FLAGS.max_steps):\n",
    "    if i % 10 == 0:  # Record summaries and test-set accuracy\n",
    "      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "      test_writer.add_summary(summary, i)\n",
    "      print('Accuracy at step %s: %s' % (i, acc))\n",
    "    else:  # Record train set summaries, and train\n",
    "      if i % 100 == 99:  # Record execution stats\n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "        summary, _ = sess.run([merged, train_step],\n",
    "                              feed_dict=feed_dict(True),\n",
    "                              options=run_options,\n",
    "                              run_metadata=run_metadata)\n",
    "        train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "        train_writer.add_summary(summary, i)\n",
    "        print('Adding run metadata for', i)\n",
    "      else:  # Record a summary\n",
    "        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "        train_writer.add_summary(summary, i)\n",
    "  train_writer.close()\n",
    "  test_writer.close()\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "  train()\n",
    "\n",
    "#argparse是一个受optparse启发的命令行解析库，功能：\n",
    "# 1. 处理可选参数和位置参数\n",
    "# 2. 生成 usage messages\n",
    "# 3. 支持派生出sub-parser\n",
    "# argparse包含以下public classes\n",
    "# 1. ArgumentParser\n",
    "# 命令行解析的主要入口点。\n",
    "# 如下例所示，使用add_argument()方法为解析器填充可选参数和位置参数的动作。\n",
    "# 然后调用parse_args()方法将命令行中的参数转换为具有属性的对象。\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--fake_data', nargs='?', const=True, type=bool,\n",
    "                      default=False,\n",
    "                      help='If true, uses fake data for unit testing.')\n",
    "  parser.add_argument('--max_steps', type=int, default=1000,\n",
    "                      help='Number of steps to run trainer.')\n",
    "  parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                      help='Initial learning rate')\n",
    "  parser.add_argument('--dropout', type=float, default=0.9,\n",
    "                      help='Keep probability for training dropout.')\n",
    "  parser.add_argument(\n",
    "      '--data_dir',\n",
    "      type=str,\n",
    "      default=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                           'tensorflow/mnist/input_data'),\n",
    "      help='Directory for storing input data')\n",
    "  parser.add_argument(\n",
    "      '--log_dir',\n",
    "      type=str,\n",
    "      default=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                           'tensorflow/mnist/logs/mnist_with_summaries'),\n",
    "      help='Summaries log directory')\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv= sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 程序相关问题"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "深度学习神经网络往往有过多的Hyperparameter需要调优，优化算法、学习率、卷积核尺寸等很多参数都需要不断调整，使用命令行参数是非常方便的。有两种实现方式，一是利用python的argparse包，二是调用tensorflow自带的app.flags实现。\n",
    "\n",
    "利用python的argparse包\n",
    "argparse是python的命令行解析工具，或者说可以在python代码中调用shell的一些命令，从而简化和系统命令之间的交互。tensorflow的一些例子中用argparse来定义一些默认命令，通常是全局变量，也是用作和系统命令之间交互的全局设置。具体编程时，argparse包的使用方法如下\n",
    "\n",
    "step 1、导入argparse模块\n",
    "import argparse\n",
    "\n",
    "step 2、创建解析器对象ArgumentParser，可以添加参数。\n",
    "description：描述程序\n",
    "parser=argparse.ArgumentParser(description=\"This is a example program \")\n",
    "add_help：默认是True，可以设置False禁用\n",
    "\n",
    "step 3、add_argument()方法，用来指定程序需要接受的命令参数\n",
    "定位参数：\n",
    "parser.add_argument(\"echo\",help=\"echo the string\")\n",
    "可选参数：\n",
    "parser.add_argument(\"--verbosity\", help=\"increase output verbosity\")\n",
    "在执行程序的时候，定位参数必选，可选参数可选。\n",
    "add_argument()常用的参数：\n",
    "dest：如果提供dest，例如dest=\"a\"，那么可以通过args.a访问该参数\n",
    "default：设置参数的默认值\n",
    "action：参数触发的动作\n",
    "store：保存参数，默认\n",
    "store_const：保存一个被定义为参数规格一部分的值（常量），而不是一个来自参数解析而来的值。\n",
    "store_ture/store_false：保存相应的布尔值\n",
    "append：将值保存在一个列表中。\n",
    "append_const：将一个定义在参数规格中的值（常量）保存在一个列表中。\n",
    "count：参数出现的次数\n",
    "parser.add_argument(\"-v\", \"--verbosity\", action=\"count\", default=0, help=\"increase output verbosity\")\n",
    "version：打印程序版本信息\n",
    "type：把从命令行输入的结果转成设置的类型\n",
    "choice：允许的参数值\n",
    "parser.add_argument(\"-v\", \"--verbosity\", type=int, choices=[0, 1, 2], help=\"increase output verbosity\")\n",
    "help：参数命令的介绍\n",
    "\n",
    "通过调用python的argparse包，调用函数parser.parse_known_args()解析命令行参数。代码运行后得到的FLAGS是一个结构体，内部参数分别为：\n",
    "\n",
    "FLAGS.data_dir\n",
    "Out[5]: '/tmp/tensorflow/mnist/input_data'\n",
    "\n",
    "FLAGS.fake_data\n",
    "Out[6]: False\n",
    "\n",
    "FLAGS.max_steps\n",
    "Out[7]: 1000\n",
    "\n",
    "FLAGS.learning_rate\n",
    "Out[8]: 0.001\n",
    "\n",
    "FLAGS.dropout\n",
    "Out[9]: 0.9\n",
    "\n",
    "FLAGS.data_dir\n",
    "Out[10]: '/tmp/tensorflow/mnist/input_data'\n",
    "\n",
    "FLAGS.log_dir\n",
    "Out[11]: '/tmp/tensorflow/mnist/logs/mnist_with_summaries'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.app.flags组件\n",
    "首先需要定义一个tf.app.flags对象，调用自带的DEFINE_string, DEFINE_boolean, DEFINE_integer, DEFINE_float设置不同类型的命令行参数及其默认值。当然，也可以在终端用命令行参数修改这些默认值。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define hyperparameters\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_boolean(\"enable_colored_log\", False, \"Enable colored log\")\n",
    "                    \"The glob pattern of train TFRecords files\")\n",
    "flags.DEFINE_string(\"validate_tfrecords_file\",\n",
    "                    \"./data/a8a/a8a_test.libsvm.tfrecords\",\n",
    "                    \"The glob pattern of validate TFRecords files\")\n",
    "flags.DEFINE_integer(\"label_size\", 2, \"Number of label size\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.01, \"The learning rate\")\n",
    "\n",
    "def main():\n",
    "    # Get hyperparameters\n",
    "    if FLAGS.enable_colored_log:\n",
    "        import coloredlogs\n",
    "        coloredlogs.install()\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    FEATURE_SIZE = FLAGS.feature_size\n",
    "    LABEL_SIZE = FLAGS.label_size\n",
    "    ...\n",
    "  return 0\n",
    "\n",
    "if __name__ == ‘__main__’:\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "这段代码采用的是tensorflow库中自带的tf.app.flags模块实现命令行参数的解析。如果用终端运行tf程序，用上述两种方式都可以，如果用spyder之类的工具，那么只有第一种方式有用，第二种方式会报错。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "其中有个tf.app.flags组件，还有个tf.app.run()函数。官网帮助文件是这么说的：\n",
    "flags module: Implementation of the flags interface.\n",
    "run(...): Runs the program with an optional 'main' function and 'argv' list.\n",
    "\n",
    "tf.app.run的源代码："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.\"\"\"Generic entry point script.\"\"\"  \n",
    "2.from __future__ import absolute_import  \n",
    "3.from __future__ import division  \n",
    "4.from __future__ import print_function  \n",
    "5.  \n",
    "6.import sys  \n",
    "7.  \n",
    "8.from tensorflow.python.platform import flags  \n",
    "9.  \n",
    "10.  \n",
    "11.def run(main=None):  \n",
    "12.  f = flags.FLAGS  \n",
    "13.  f._parse_flags()  \n",
    "14.  main = main or sys.modules['__main__'].main  \n",
    "15.  sys.exit(main(sys.argv))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "也就是处理flag解析，然后执行main函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Namespace(data_dir='/tmp\\\\tensorflow/mnist/input_data', dropout=0.9, fake_data=False, learning_rate=0.001, log_dir='/tmp\\\\tensorflow/mnist/logs/mnist_with_summaries', max_steps=1000), ['-f', 'C:\\\\Users\\\\THINK\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-46006dad-bf18-4047-9000-356a70288bcc.json'])\n"
     ]
    }
   ],
   "source": [
    "print(parser.parse_known_args()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整个程序流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with tf.name_scope限定命名空间。定义输入x、y placeholder。输入一维数据变形28x28图片储存到tensor，tf.summary.image汇总图片数据TensorBoard展示。\n",
    "\n",
    "定义神经网络模型参数初始化方法，权重用truncated_normal初始化，偏置赋值0.1。\n",
    "\n",
    "定义Variable变量数据汇总函数，计算Variable mean、stddev、max、min，tf.summary.scalar记录、汇总。tf.summary.histogram记录变量var直方图数据。\n",
    "\n",
    "设计MLP多层神经网络训练数据，每一层汇总模型参数数据。定义创建一层神经网络数据汇总函数nn_layer。输入参数，输入数据input_tensor、输入维度input_dim、输出维度output_dim、层名称layer_name。激活函数act用ReLU。初始化神经网络权重、偏置，用variable_summaries汇总variable数据。输入，矩阵乘法，加偏置，未激活结果用tf.summary.histogram统计直方图。用激活函数后，tf.summary.histogram再统计一次。\n",
    "\n",
    "nn_layer创建一层神经网络，输入维度图片尺寸28x28=784，输出维度隐藏节点数500。创建Dropout层，用tf.summary.scalar记录keep_prob。用nn_layer定义神经网络输出层，输入维度为上层隐含节点数500,输出维度类别数10,激活涵数全等映射identity。\n",
    "\n",
    "tf.nn.softmax_cross_entropy_with_logits()对前面输出层结果Softmax处理，计算交叉熵损失cross_entropy。计算平均损失，tf.summary.scalar统计汇总。\n",
    "\n",
    "Adma优化器优化损失。统计预测正确样本数，计算正确率accury， tf.summary.scalar统计汇总accuracy。\n",
    "\n",
    "tf.summary.merger_all()获取所有汇总操作。定义两个tf.summary.FileWriter(文件记录器)在不同子目录，分别存放训练和测试日志数据。Session计算图sess.graph加入训练过程记录器，TensorBoard GRAPHS窗口展示整个计算图可视化效果。tf.global_variables_initializer().run()初始化全部变量。\n",
    "\n",
    "定义feed_dict损失函数。先判断训练标记，True，从mnist.train获取一个batch样本，设置dropout值；False，获取测试数据，设置keep_prob 1,没有dropout效果。\n",
    "\n",
    "实际执行具体训练、测试、日志记录操作。tf.train.Saver()创建模型保存器。进入训练循环，每隔10步执行merged(数据汇总)、accuracy(求测试集预测准确率)操作，test_writer.add_sumamry将汇总结果summary和循环步数i写入日志文件。每隔100步，tf.RunOptions定义TensorFlow运行选项，设置trace_lever FULL_TRACE。tf.RunMetadata()定义TensorFlow运行元信息，记录训练运算时间和内存占用等信息。执行merged数据汇总操作，train_step训练操作，汇总结果summary、训练元信息run_metadata添加到train_writer。执行merged、train_step操作，添加summary到train_writer。所有训练全部结束，关闭train_writer、test_writer。\n",
    "\n",
    "切换Linux命令行，执行TensorBoard程序，--logdir指定TensorFlow日志路径，TensorBoard自动生成所有汇总数据可视化结果。\n",
    "tensorboard --logdir=/tmp/tensorflow/mnist/logs/mnist_with_summaries\n",
    "复制网址到浏览器。\n",
    "\n",
    "打开标量SCALARS窗口，打开accuracy图表。调整Smoothing参数，控制曲线平滑处理，数值越小越接近实际值，波动大；数值越大曲线越平缓。图表下方按钮放大图片，右边按钮调整坐标轴范围。\n",
    "\n",
    "切换图像IMAGES窗口，可以看到所有tf.summary.image()汇总数据。\n",
    "\n",
    "计算图GRAPHS窗口，整个TensorFlow计算图结构。网络forward inference流程，backward训练更新参数流程。实线代表数据依赖关系，虚线代表控制条件依赖关系。节点窗口，看属性、输入、输出及tensor尺寸。\n",
    "\n",
    "\"+\"按钮，展示node内部细节。所有同一命名空间节点被折叠一起。右键单击节点选择删除。\n",
    "\n",
    "切换配色风络，基于结构，同结构节点同颜色；基于运算硬件，同运算硬件节点同颜色。\n",
    "\n",
    "Session runs，选择run_metadata训练元信息。\n",
    "\n",
    "切换DISTRIBUTIONS窗口，看各个神经网络层输出分布，激活函数前后结果。看看有没有被屏蔽节点(dead neurons)。转为直方图。\n",
    "\n",
    "EMBEDDINGS窗口，降维嵌入向量可视化效果。tf.save.Saver保存整个模型，TensorBoard自动对模型所有二维Variable可视化(只有Variable可以被保存，Tensor不行)。选择T-SNE或PCA算法对数据列(特征)降维，在3D、2D坐标可视化展示。对Word2Vec计算或Language Model非常有用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/eBbEDRsCmv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
