{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow-Estimator-自定义估算器\n",
    "这篇文章介绍自定义一个估算器（分类器）Estimator的完整流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义Custom Estimator和预制Pre-made Estimator\n",
    "在上面iris的案例中我们使用了tensorflow里面自带的深度神经网络分类器tf.estimator.DNNClassifie。这些tensorflow自带的estimator称为预制估算器Pre-made Estimator（预创建的Estimator）。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[10, 10],\n",
    "    n_classes=3,\n",
    "    model_dir=models_path,\n",
    "    config=ckpt_config) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow允许我们自己创建更加灵活的Custom Estimator。自定义Estimator是tf.estimator.Estimator()方法生成，能够像预制Estimator一样使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 结构概览\n",
    "\n",
    "从表面看，我们的Estimator应该具有DNNClassifier一样的功能\n",
    "\n",
    "创建的时候接收一些参数，如feature_columns、hidden_units、n_classes等\n",
    "\n",
    "具有train()、evaluate()、predict()三个方法用来训练、评价、预测\n",
    "\n",
    "如上所说，我们使用 tf.estimator.Estimator()方法来生成自定义Estimator，它的语法格式是\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.estimator.Estimator(\n",
    "    model_fn, #模型函数\n",
    "    model_dir=None, #存储目录\n",
    "    config=None, #设置参数对象\n",
    "    params=None, #超参数，将传递给model_fn使用\n",
    "    warm_start_from=None #热启动目录路径\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "模型函数model_fn是唯一没有默认值的参数，它也是自定义Estimator最关键的部分，包含了最核心的\n",
    "算法。model_fn需要一个能够进行运算的函数，它的样子应该长成这样\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "my_model(\n",
    "  features, #输入的特征数据\n",
    "  labels, #输入的标签数据\n",
    "  mode, #train、evaluate或predict\n",
    "  params #超参数，对应上面Estimator传来的参数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络层Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入层Input Layer，数据从这里进入\n",
    "\n",
    "隐藏层Hidden Layer，2个，每层包含多个节点，数据流经这里，被推测规律\n",
    "\n",
    "输出层Output Layer，将推测的结果整理显示出来\n",
    "\n",
    "我们并不需要手工实现隐藏层的算法和工作原理，Tensorflow已经为我们设计好。我们需要的只是创建这些神经网络层，并确保它们按照正常的顺序连接起来，至于其中如何推算演绎的魔法就完全交给tensorflow就可以了。\n",
    "\n",
    "mode_fn需要完成的就是创建和组织这些神经层。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编写model_fn\n",
    "对应我们创建Estimator时候的参数"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[10, 10],\n",
    "    n_classes=3,\n",
    "    model_dir=models_path,\n",
    "    config=ckpt_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些参数都会被Estimator打包放在params超参数中，传递给model_fn，所以我们用下面的代码在model_fn内创建网络层"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#自定义模型函数\n",
    "def my_model_fn(features,labels,mode,params):\n",
    "    #输入层,feature_columns对应Classifier(feature_columns=...)\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    \n",
    "    #隐藏层,hidden_units对应Classifier(unit=[10,10])，2个各含10节点的隐藏层\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "    \n",
    "    #输出层，n_classes对应3种鸢尾花\n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入层Input Layer\n",
    "Input Layer把输入的数据features填充到特征列params['feature_column']里面,稍后它会被继续传递到隐藏层hidden layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " net = tf.feature_column.input_layer(features, params['feature_columns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隐藏层Hidden Layer\n",
    "我们使用循环为hidden_unit列表([10,10])创建了2个隐藏图层，每个图层的神经元节点unit都等于10."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出层Output Layer\n",
    "仍然是链条的延续！\n",
    "\n",
    "但是activation这里改为了None，不再激活后续的部分，所以输出层就是链条的终点。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logits = tf.layers.dense(net, params['n_classes'], activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练train、评价evaluate和预测predict\n",
    "前面我们知道，自定义的估算分类器必须能够用来执行my_classifier.train()、my_classifier.evaluate()、my_classifier.predict()三个方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 但实际上，它们都是model_fn这一个函数的分身！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面出现的model_fn语法："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "my_model(\n",
    "  features, #输入的特征数据\n",
    "  labels, #输入的标签数据\n",
    "  mode, #train、evaluate或predict\n",
    "  params #超参数，对应上面Estimator传来的参数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三个参数mode，如果它等于\"TRAIN\"我们就执行训练："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#示意代码\n",
    "my_model(..,..,\"TRAIN\",...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是“EVAL”就执行评价，“PREDICT”就执行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测Predict\n",
    "因为预测最后我们需要返回花的种类label，还希望知道这个预测有多精确，所以在预测部分的代码里面，首先取到三种花可能性最大的一个predicted_classes即[-1.3,2.6,-0.9]中的2.6；然后把它转成列表格式[[2.6]];同时把logit得到的[-1.3,2.6,-0.9]转化为表示0～1可能性的小数[0.01926995 0.95198274 0.02874739]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predicted_classes = tf.argmax(logits, 1) #预测的结果中最大值即种类\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],#拼成列表[[3],[2],[1]]格式\n",
    "            'probabilities': tf.nn.softmax(logits), #把[-1.3,2.6,-0.9]规则化到0~1范围,表示可能性\n",
    "            'logits': logits,#[-1.3,2.6,-0.9]\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 注意最后一句，我们返回return的是一个EstimatorSpec对象，下面的训练predict和评价evaluate也都返回EstimatorSpec形式的对象，但是参数不同，请留意。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用以下代码在单独文件测试tf.newaxis和tf.nn.softmax对数据转化的作用"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a=tf.constant([2.6],name='a')\n",
    "b=a[:,tf.newaxis]\n",
    "\n",
    "a2=tf.constant([-1.3,2.6,-0.9],name='a')\n",
    "b2= tf.nn.softmax(a2)\n",
    "\n",
    "with tf.Session() as session:   \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    print(session.run(b))\n",
    "    print(session.run(b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[2.6]]\n",
    "[0.01926995 0.95198274 0.02874739]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数是Tensorflow中神经网络的重要概念，简单说，它能够计算出我们模型的偏差程度，结果越大，我们的模型就偏差越大、离正确也远、也越不准确、越糟糕。\n",
    "\n",
    "为了降低损失，我们可以使用更多更好的数据，还可以设计更好的优化方法，来优化改进模型，让损失变为最小。\n",
    "\n",
    "训练神经网络模型的目标就是把偏差损失降为最小，机器学习就是一批一批数据反复分析计算反复尝试，不断的利用优化方法，想尽办法把Loss的值降到最小的过程。\n",
    "\n",
    "优化方法设计的越好好，损失也就越少，精度也就越高。\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们使用了tensorflow提供的稀疏柔性最大交叉熵sparse_softmax_cross_entropy来计算损失程度，它对于分类问题很有效，DNNClassifier也使用了这个方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练Train\n",
    "我们在训练部分代码中，创建了优化器optimizer，然后使用它尝试将我们的损失函数loss变为最小minimize："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=0.1) #用它优化损失函数，达到损失最少精度最高\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())  #执行优化！\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评价Evaluate\n",
    "我们使用下面的代码来评价预测结果prediction和test数据中植物学家标记的数据是否足够吻合:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                               predictions=predicted_classes,\n",
    "                               name='acc_op') #计算精度\n",
    "metrics = {'accuracy': accuracy} #返回格式\n",
    "tf.summary.scalar('accuracy', accuracy[1]) #仅为了后面图表统计使用\n",
    "if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完整程序如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL =  \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "CSV_COLUMN_NAMES =  ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "def load_data(label_name = 'Species'):\n",
    "    train_path = tf.keras.utils.get_file(\n",
    "        fname=os.path.basename(TRAIN_URL),\n",
    "        origin=TRAIN_URL)\n",
    "    train = pd.read_csv(\n",
    "        train_path,\n",
    "        names = CSV_COLUMN_NAMES,\n",
    "        header = 0\n",
    "    )\n",
    "    train_features,train_label = train,train.pop(label_name)\n",
    "    \n",
    "    test_path = tf.keras.utils.get_file(\n",
    "        fname=os.path.basename(TEST_URL),\n",
    "        origin=TEST_URL\n",
    "    )\n",
    "    test = pd.read_csv(\n",
    "        test_path,\n",
    "        names = CSV_COLUMN_NAMES,\n",
    "        header = 0\n",
    "    )\n",
    "    test_features,test_label = test,test.pop(label_name)\n",
    "    \n",
    "    return (train_features,train_label),(test_features,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
      "0          6.4         2.8          5.6         2.2\n",
      "1          5.0         2.3          3.3         1.0\n",
      "2          4.9         2.5          4.5         1.7\n",
      "3          4.9         3.1          1.5         0.1\n",
      "4          5.7         3.8          1.7         0.3\n"
     ]
    }
   ],
   "source": [
    "(train_x,train_y),(test_x,test_y) = load_data()\n",
    "print(train_x[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编写model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(features,labels,mode,params):\n",
    "    #输入层,feature_columns对应Classifier(feature_columns=...)\n",
    "    net = tf.feature_column.input_layer(features,params['feature_columns'])\n",
    "    \n",
    "    #隐藏层,hidden_units对应Classifier(unit=[10,10])，2个各含10节点的隐藏层\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net,units=units,activation=tf.nn.relu)\n",
    "    \n",
    "    #输出层，n_classes对应3种鸢尾花\n",
    "    logits = tf.layers.dense(net,units=params['n_classes'],activation=None)\n",
    "    #预测\n",
    "    predicted_classes = tf.argmax(logits,1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids':predicted_classes[:,tf.newaxis],\n",
    "            'probabilities':tf.nn.softmax(logits),\n",
    "            'logits':logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode,predictions=predictions)\n",
    "    #损失函数\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels,logits=logits)\n",
    "    \n",
    "    #训练\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=0.1) #用它优化损失函数，达到损失最少精度最高\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())  #执行优化！\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "   #评价\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op') #计算精度\n",
    "    metrics = {'accuracy': accuracy} #返回格式\n",
    "    tf.summary.scalar('accuracy', accuracy[1]) #仅为了后面图表统计使用\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'my_dir/iris', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000017AC4932DA0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=my_model,\n",
    "    model_dir = 'my_dir/iris',\n",
    "    params = {\n",
    "        'feature_columns': my_feature_columns,\n",
    "        'hidden_units':[10,10],\n",
    "        'n_classes':3,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from my_dir/iris\\model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into my_dir/iris\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.022829015, step = 3001\n",
      "INFO:tensorflow:global_step/sec: 520.567\n",
      "INFO:tensorflow:loss = 0.04111736, step = 3101 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 861.816\n",
      "INFO:tensorflow:loss = 0.038586494, step = 3201 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 961.877\n",
      "INFO:tensorflow:loss = 0.01956015, step = 3301 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 961.167\n",
      "INFO:tensorflow:loss = 0.04609591, step = 3401 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 924.674\n",
      "INFO:tensorflow:loss = 0.059080057, step = 3501 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 961.416\n",
      "INFO:tensorflow:loss = 0.049300063, step = 3601 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 962.65\n",
      "INFO:tensorflow:loss = 0.030422237, step = 3701 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 960.606\n",
      "INFO:tensorflow:loss = 0.044866804, step = 3801 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 998.848\n",
      "INFO:tensorflow:loss = 0.045474336, step = 3901 (0.100 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into my_dir/iris\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.026304847.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x17ac4932eb8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#针对训练的喂食函数\n",
    "batch_size=100\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size) #每次随机调整数据顺序\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "#开始训练\n",
    "classifier.train(\n",
    "    input_fn=lambda:train_input_fn(train_x, train_y, 100),\n",
    "    steps=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评价模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-06-00:25:49\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from my_dir/iris\\model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-06-00:25:50\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.96666664, global_step = 4000, loss = 0.05673013\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4000: my_dir/iris\\model.ckpt-4000\n",
      "\n",
      "Test set accuracy: 0.967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def eval_input_fn(features,labels,batch_size):\n",
    "    features = dict(features)\n",
    "    inputs = (features,labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "#评价结果\n",
    "eval_result = classifier.evaluate(input_fn=lambda:eval_input_fn(test_x,test_y,batch_size))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter features: SepalLength,SepalWidth,PetalLength,PetalWidth\n"
     ]
    }
   ],
   "source": [
    "#支持100次循环对新数据进行分类预测\n",
    "for i in range(0,100):\n",
    "    print('\\nPlease enter features: SepalLength,SepalWidth,PetalLength,PetalWidth')\n",
    "    a,b,c,d = map(float, input().split(',')) #捕获用户输入的数字\n",
    "    predict_x = {\n",
    "        'SepalLength': [a],\n",
    "        'SepalWidth': [b],\n",
    "        'PetalLength': [c],\n",
    "        'PetalWidth': [d],\n",
    "    }\n",
    "    \n",
    "    #进行预测\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=lambda:eval_input_fn(predict_x,\n",
    "                                      labels=[0,],\n",
    "                                      batch_size=batch_size))    \n",
    "\n",
    "    #预测结果是数组，尽管实际我们只有一个\n",
    "    for pred_dict in predictions:\n",
    "        class_id = pred_dict['class_ids'][0]\n",
    "        probability = pred_dict['probabilities'][class_id]\n",
    "        print(SPECIES[class_id],100 * probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
